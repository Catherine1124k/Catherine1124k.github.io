---
layout: post
title: "Week 6"
date: 2021-02-18
---

What if it is not plausible that a response is normally distributed? You may want to construct a model to predict whether a prospective student will enroll at a school or model the lifetimes of patients following a particular surgery. In the first case you have a binary response (enrolls (1) or does not enroll (0)), and in the second case you are likely to have very skewed data with many similar values and a few hardy souls with extremely long survival. These responses are not expected to be normally distributed; other distributions will be needed to describe and model binary or lifetime data. Non-normal responses are encountered in a large number of situations. Luckily, there are quite a few possibilities for models. In this chapter we begin with some general definitions, terms, and notation for different types of distributions with some examples of applications. We then create new random variables using combinations of random variables (see Guided Exercises).

A discrete random variable has a countable number of possible values; for example, we may want to measure the number of people in a household or the number of crimes committed on a college campus. With discrete random variables, the associated probabilities can be calculated for each possible value using a probability mass function (pmf). A pmf is a function that calculates P(Y=y), given each variable’s parameters.

Consider the event of flipping a (possibly unfair) coin. If the coin lands heads, let’s consider this a success and record Y=1. A series of these events is a Bernoulli process, independent trials that take on one of two values (e.g., 0 or 1). These values are often referred to as a failure and a success, and the probability of success is identical for each trial. Suppose we only flip the coin once, so we only have one parameter, the probability of flipping heads, p. If we know this value, we can express P(Y=1)=p and P(Y=0)=1−p.
In general, if we have a Bernoulli process with only one trial, we have a binary distribution (also called a Bernoulli distribution) where

P(Y=y)=py(1−p)1−yfory=0,1.(3.1)

If Y∼Binary(p), then Y has mean E(Y)=p and standard deviation SD(Y)=√p(1−p).

Example 1: Your playlist of 200 songs has 5 which you cannot stand. What is the probability that when you hit shuffle, a song you tolerate comes on?

Assuming all songs have equal odds of playing, we can calculate p=200−5200=0.975 , so there is a 97.5% chance of a song you tolerate playing, since P(Y=1)=.9751∗(1−.975)^0.

We can extend our knowledge of binary random variables. Suppose we flipped an unfair coin n times and recorded Y, the number of heads after n flips. If we consider a case where p=0.25 and n=4, then here P(Y=0) represents the probability of no successes in 4 trials, i.e., 4 consecutive failures. The probability of 4 consecutive failures is P(Y=0)=P(TTTT)=(1−p)4=0.754. When we consider P(Y=1), we are interested in the probability of exactly 1 success anywhere among the 4 trials. There are (41)=4 ways to have exactly 1 success in 4 trials, so P(Y=1)=(41)p1(1−p)4−1=(4)(0.25)(0.75)3. In general, if we carry out a sequence of n Bernoulli trials (with probability of success p) and record Y, the total number of successes, then Y

follows a binomial distribution, where

P(Y=y)=(ny)p^y(1−p)^(n−y) for y=0,1,…,n.

Suppose we are to perform independent, identical Bernoulli trials until the first success. If we wish to model Y, the number of failures before the first success, we can consider the following pmf:

P(Y=y)=(1−p)^yp for y=0,1,…,∞.

We can think about this function as modeling the probability of y failures, then 1 success. In this case, Y follows a geometric distribution with E(Y)=(1−p)/p and SD(Y)=√1−pp2.

In all previous random variables, we considered a Bernoulli process, where the probability of a success remained constant across all trials. What if this probability is dynamic? The hypergeometric random variable helps us address some of these situations. Specifically, what if we wanted to select n items without replacement from a collection of N objects, m of which are considered successes? In that case, the probability of selecting a “success” depends on the previous selections. If we model Y, the number of successes after n selections, Y follows a hypergeometric distribution

Sometimes, random variables are based on a Poisson process. In a Poisson process, we are counting the number of events per unit of time or space and the number of events depends only on the length or size of the interval. We can then model Y, the number of events in one of these sections with the Poisson distribution.

A continuous random variable can take on an uncountably infinite number of values. With continuous random variables, we define probabilities using probability density functions (pdfs). Probabilities are calculated by computing the area under the density curve over the interval of interest.

Suppose we have a Poisson process with rate λ, and we wish to model the wait time Y until the first event. We could model Y using an exponential distribution.

Once again consider a Poisson process. When discussing exponential random variables, we modeled the wait time before one event occurred. If Y represents the wait time before r events occur in a Poisson process with rate λ, Y follows a gamma distribution .

You have already at least informally seen normal random variables when evaluating LLSR assumptions. To recall, we required responses to be normally distributed at each level of X. Like any continuous random variable, normal (also called Gaussian) random variables have their own pdf, dependent on μ, the population mean of the variable of interest, and σ, the population standard deviation. 

So far, all of our continuous variables have had no upper bound. If we want to limit our possible values to a smaller interval, we may turn to a beta random variable. 

We have spent most of this chapter discussing probability distributions that may come in handy when modeling. The following distributions, while rarely used in modeling, prove useful in hypothesis testing as certain commonly used test statistics follow these distributions.
